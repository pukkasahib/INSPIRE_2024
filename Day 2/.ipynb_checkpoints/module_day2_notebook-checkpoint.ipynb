{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "# Day 2: Introduction to Machine Learning in Python\n",
    "---\n",
    "\n",
    "## 1. Introduction <a id='l_overview'></a>\n",
    "\n",
    "The goal of today's lecture is to present unsupervised Machine Learning. We will learn about the most typical machine learning problems, such as dimensionality reduction, and how to approach these using the Python programmming language. These are the important concepts that we will cover:\n",
    "\n",
    "- [Machine Learning](#l_ml)\n",
    "- [Data sets](#l_ds)\n",
    "- [Dimensionality reduction](#l_dr) \n",
    "- [Principal Component Analysis (PCA)](#l_pca)\n",
    "- [Multidimensional Scaling (MDS)](#l_mds)\n",
    "- [Other dimensionality reduction techniques](#l_other)\n",
    "\n",
    "\n",
    "## 2. Machine Learning <a id='l_ml'></a>\n",
    "\n",
    "Below is the outline of the field with specific algorithms:\n",
    "\n",
    "1. **Unsupervised Learning** - there is no correct input/output pair \n",
    "    - *Clustering*\n",
    "        - K-Means\n",
    "        - Hierarchical\n",
    "        - Spectral\n",
    "    - *Dimensionality reduction*\n",
    "        - Principal Components Analysis (PCA)\n",
    "        - Multidimensional Scaling (MDS)\n",
    "        - Stochastic Neighbour Embedding (t-SNE)\n",
    "        - Uniform Manifold Approximation and Projection (UMAP)\n",
    "        \n",
    "        \n",
    "2. **Supervised Learning** - there is a correct input/output pair\n",
    "    - *Regression*\n",
    "        - Curve fitting\n",
    "        - Linear regression \n",
    "    - *Classification*\n",
    "        - Linear Classifiers (Support Vector Machines, Logistic regression)\n",
    "        - Decision Trees\n",
    "        - Neural Networks\n",
    "        \n",
    "        \n",
    "3. **Reinforcement Learning** - is an area concerned with how software agents have to take actions in an environment so as to maximize some cumulative reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating data sets\n",
    "\n",
    "Setup:\n",
    "- Suppose one has $p$ samples of N-dimensional data points, $x_i\\in\\mathbb{R}^N$\n",
    "- Store these samples columnwise as $X\\in\\mathbb{R}^{p\\,\\times\\,N}$\n",
    "- We call this the original data matrix, or simply the data\n",
    "- Assumption: there is a meaningful metric (e.g. Euclidean distance) on the data space (high dim)\n",
    "- Assumption: there is a meaningful metric (e.g. Euclidean distance) on the latent space (low dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.style.use('seaborn-darkgrid')\n",
    "# plt.style.use('seaborn-poster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Generate linear data with noise (2-dimensional data set) with 100 points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_x = np.random.uniform(0,10, size=(100,))\n",
    "raw_data_y = 0.5 * raw_data_x + np.random.normal(0,1,len(raw_data_x))\n",
    "\n",
    "X_2d = np.empty((100, 2))\n",
    "X_2d[:,0] = raw_data_x\n",
    "X_2d[:,1] = raw_data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=250, figsize=(3,3))\n",
    "plt.scatter(X_2d[:,0], X_2d[:,1], marker='.', color='steelblue', edgecolor='k', lw=0.5)\n",
    "plt.xlabel(r'$X$')\n",
    "plt.ylabel(r'$Y$')\n",
    "plt.title('Two-dimensional dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look how the data looks like (first 10 points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Load high-dimensional data from the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. \n",
    "![](pics/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take only 1000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/mnist_test.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize what these data points represent (digital images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 20 digits\n",
    "fig, axes = plt.subplots(2, 10, figsize=(16, 6))\n",
    "for i, j in enumerate(np.random.choice(np.arange(1000), size=20)):\n",
    "    image = np.array(df.iloc[j, 1:]).reshape(28,28)\n",
    "    label = np.array(df.iloc[j, 0])\n",
    "    axes[i//10, i%10].imshow(image, cmap='gray');\n",
    "    axes[i//10, i%10].axis('off')\n",
    "    axes[i//10, i%10].set_title(f\"digit: {label}\")\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will only use the first 1000 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,) (1000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df.iloc[:1000, 1:]).reshape(-1, 28, 28)\n",
    "Y = np.array(df.iloc[:1000, 0])\n",
    "print(Y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[567], cmap='gray')\n",
    "print(Y[567])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to convert each data point (picture with a handwritten digit) back to a vector which dimensionality is 28x28 = 784. i.e. to make each 28x28 matrix a flat vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.reshape(1000, 784)\n",
    "X[567].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - we have two data sets:\n",
    "- 2-dimensional data set with 100 points\n",
    "- 784-dimensional data set with 1000 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality reduction <a id='l_dr'></a>\n",
    "\n",
    "Dimensionality reduction is a technique used in machine learning and data analysis to reduce the number of input features or variables of a dataset while still retaining the important information. This is done by projecting the high-dimensional data onto a lower-dimensional space, while preserving the relevant characteristics of the original data.\n",
    "\n",
    "The main goal of dimensionality reduction is to simplify the dataset and make it more manageable for analysis, visualization, and modeling. It also helps to reduce the risk of overfitting and improve the performance of machine learning models by removing irrelevant or redundant features.\n",
    "\n",
    "There are two main types of dimensionality reduction:\n",
    "\n",
    "- Feature selection: In this method, a subset of the original features is selected based on some criteria, such as correlation or importance.\n",
    "\n",
    "- Feature extraction: In this method, a new set of features is created by transforming the original features into a lower-dimensional space using techniques such as principal component analysis (PCA), singular value decomposition (SVD), or t-distributed stochastic neighbor embedding (t-SNE).\n",
    "\n",
    "Overall, dimensionality reduction is a powerful tool for reducing the complexity of large datasets while still preserving the essential information needed for effective analysis and modeling. You can select a subset of original variables, or find a linear or nonlinear combination of features, or make a projection to lower dimensions. \n",
    "\n",
    "![](pics/dr.png)\n",
    "\n",
    "\n",
    "Methods:\n",
    "- **Principal Components Analysis (PCA)** - linear method to extract dimensions with the highest variance\n",
    "- **Multidimensional Scaling (MDS)** - nonlinear method to project in lower dimensions by saving pairwise distances\n",
    "- **Stochastic Neighbour Embedding (t-SNE)** - making an embedding in lower dimensions by conserving distribution of distances \n",
    "- **Uniform Manifold Approximation and Projection (UMAP)** - projecting the data on manifold into fewer dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Principal Component Analysis <a id='l_dr'></a>\n",
    "\n",
    "### **Math**:\n",
    "\n",
    "- **PCA goal**: Find orthogonal transformation $W$ of the centered data $X_c$ (i.e. $Y=WX_c$) such that variance along subsequent components is maximized (i.e. most variance along first, the second most variance is along the second, etc.); \n",
    "- Note that $X_c$ is $p \\times N$, $W$ is $N \\times N$, $Y$ is $p \\times N$, principal components are the columns of $W$.\n",
    "- Principal components of $X_c$ are typically found via eigendecomposition of covariance matrix $X_c^T X_c$ .\n",
    "- The PCA embedding is $Y=U^T X_c$, where $U$ stores columnwise eigenvectors of $X_c^T X_c$ in decreasing order (by eigenvalue).\n",
    "\n",
    "### Compute principle components via eigenvectors of covariance matrix\n",
    "\n",
    "1. Center data set, i.e. first subtract the mean of the dataset from the dataset.\n",
    "2. Compute the covariance matrix $X_c^T X_c$.\n",
    "3. Compute eigenvectors of $X_c^T X_c$ and order them in terms of decreasing eigenvalues.\n",
    "4. Transform the data using the eigenvectors stored columnwise in a matrix $U$ by $Y=U^T X_c$.\n",
    "5. Compare our step-by-step method to the pythonic library PCA implementation\n",
    "\n",
    "## Now we apply PCA method to 2d dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "\n",
    "X_2d_centered = X_2d - np.mean(X_2d, axis=0)\n",
    "\n",
    "# Visualize\n",
    "fig, (a0, a1) = plt.subplots(1, 2, figsize=(16, 8), dpi=250)\n",
    "\n",
    "a0.scatter(X_2d[:,0], X_2d[:,1], label='original data', color='steelblue', edgecolor='k', lw=0.5,)\n",
    "a1.scatter(X_2d_centered[:,0], X_2d_centered[:,1], label='centered data', color='deepskyblue', edgecolor='k', lw=0.5,)\n",
    "a0.legend()\n",
    "a1.legend()\n",
    "\n",
    "a0.set_xlabel(r'$X$')\n",
    "a0.set_ylabel(r'$Y$')\n",
    "\n",
    "a1.set_xlabel(r'$X$')\n",
    "a1.set_ylabel(r'$Y$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is now centered on the origin.\n",
    "Now compute the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix:\n",
      "[[787.58308328 372.79326446]\n",
      " [372.79326446 275.21939021]]\n"
     ]
    }
   ],
   "source": [
    "# Step 2.\n",
    "Cov = np.dot(np.transpose(X_2d_centered), X_2d_centered)\n",
    "print(\"Covariance matrix:\")\n",
    "print(Cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3\n",
    "eigvals, W = np.linalg.eig(Cov)\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(eigvals)\n",
    "print(\"\\nEigenvectors (columns)\")\n",
    "print(W)\n",
    "\n",
    "print(\"\\nCheck that eigenvectors are orthogonal by computing their inner product (<w1,w2>=0):\")\n",
    "print(np.dot(W[:,0],W[:,1]))\n",
    "\n",
    "print('\\nVariance in the first principal component: {}'.format(eigvals[0]/np.sum(eigvals)))\n",
    "print('Variance in the second principal component: {}'.format(eigvals[1]/np.sum(eigvals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the eigenvectors in comparison to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=250, figsize=(4,4))\n",
    "\n",
    "plt.scatter(X_2d_centered[:,0], X_2d_centered[:,1], color='steelblue', edgecolor='k', lw=0.5, label='data points')\n",
    "plt.xlabel(r'$X$')\n",
    "plt.ylabel(r'$Y$')\n",
    "plt.title('Two-dimensional dataset')\n",
    "\n",
    "plt.plot([0, W[0][1]*3],[0, W[1][1]*3],'k',lw=4, label='eigenvector 2', ls='--')\n",
    "plt.plot([0, W[0][0]*5],[0, W[1][0]*5],'k',lw=4, label='eigenvector 1')\n",
    "plt.axis('equal')\n",
    "plt.legend(fancybox=True, shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply the transformation to the data and plot the data in the new space. We flip the matrix W and corresponding eigenvalues so that they are ordered the same way as in the theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4\n",
    "\n",
    "#eigvals = eigvals[::-1]\n",
    "#print(eigvals)\n",
    "\n",
    "# Applying transformation\n",
    "X_2d_transformed = np.dot(X_2d_centered, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=250, figsize=(4,4))\n",
    "plt.scatter(X_2d_transformed[:,0], X_2d_transformed[:,1], color='tomato', edgecolor='k', lw=0.5, label='data points')\n",
    "plt.title('Two-dimensional dataset transformed with manual PCA', fontsize=5)\n",
    "plt.xticks(fontsize=5)\n",
    "plt.yticks(fontsize=5)\n",
    "plt.xlabel('PC 1', fontsize=5)\n",
    "plt.ylabel('PC 2', fontsize=5)\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-4, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our naive implementation to the PCA implementation from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "\n",
    "skl_PCA = PCA(n_components = 2).fit(X_2d) # fit the data to receive eigenvectors of covariance matrix\n",
    "skl_X_2d_transformed = skl_PCA.transform(X_2d) # apply a transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, (a0, a1) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "a0.scatter(skl_X_2d_transformed[:,0], skl_X_2d_transformed[:,1], label='sklearn PCA', color='steelblue', edgecolor='k',lw=0.5)\n",
    "a1.scatter(X_2d_transformed[:,0], X_2d_transformed[:,1], label='our own PCA', color='tomato', edgecolor='k',lw=0.5)\n",
    "a0.legend()\n",
    "a1.legend()\n",
    "\n",
    "a0.set_xlabel(r'$PC 1$')\n",
    "a0.set_ylabel(r'$PC 2$')\n",
    "\n",
    "a1.set_xlabel(r'$PC 1$')\n",
    "a1.set_ylabel(r'$PC 2$')\n",
    "\n",
    "a0.set_xlim(-8, 8)\n",
    "a0.set_ylim(-4, 4)\n",
    "\n",
    "a1.set_xlim(-8, 8)\n",
    "a1.set_ylim(-4, 4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks (almost) identical, up to 180 degree rotation! \n",
    "\n",
    "We will now truncate the data to one dimension and see how it looks. It's called a simple PCA dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skl_PCA.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that >90% of variance is in the first principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(skl_X_2d_transformed[:,0] , np.zeros(shape=skl_X_2d_transformed[:,0].shape), label='sklearn PCA', color='lime', edgecolor='w', lw=2, marker='o')\n",
    "plt.ylim((-6,6))\n",
    "plt.xlim((-8,8))\n",
    "plt.legend()\n",
    "plt.yticks([])\n",
    "plt.xlabel('PC 1')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1\n",
    "\n",
    "Perform principal component analysis on the 1000 points of 784-dimensional MNIST dataset using `sklearn`.\n",
    "The dataset is already in the memory of the Jupyter Notebook under variable `X`. `Y` contains the label of each handwritten digit, i.e. the number, or the class. [Documentation on sklearn PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) \n",
    "\n",
    "784-dimensional dataset has 784 principal components (PC). \n",
    "\n",
    "1. Plot the percent variance contained in each PC vs PC number. **Hint**: variable `explained_variance_ratio_` may be useful.\n",
    "2. Now plot cumulative percent variance vs number of PC components used. Decide how many PC you need to capture 90% of total variance.\n",
    "2. Use the first two principal components to represent MNIST data set in two dimensions on a scatter plot. Each mnist digit will now be represented as a point. \n",
    "3. Show the image of first two principal eigenvectors, rescaled as 28x28. **Hint**: variable `components_` and function `reshape()` might be useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multidimensional Scaling (MDS) <a id='l_mds'></a>\n",
    "\n",
    "#### Metric MDS\n",
    "Setup:\n",
    "- Suppose one has $p$ samples of N-dimensional data points, $x_i\\in\\mathbb{R}^N$\n",
    "- Store these samples rowwise as $X\\in\\mathbb{R}^{p\\,\\times\\,N}$\n",
    "- We call this the original data matrix, or simply the data\n",
    "- Assumption: there is a meaningful metric (e.g. Euclidean distance) on the data space (high dim)\n",
    "- Assumption: there is a meaningful metric (e.g. Euclidean distance) on the latent space (low dim)\n",
    "\n",
    "Goal:\n",
    "- Given N-dim data $X$, a metric $d(\\cdot,\\cdot)$ on $\\mathbb{R}^N$, a target dimension $k<N$, and a metric $g(\\cdot,\\cdot)$ on $\\mathbb{R}^k$\n",
    "- Find an embedding $Y\\in\\mathbb{R}^{k\\,\\times\\,p}$ (i.e. a $y_i\\in\\mathbb{R}^k$ for each $x_i\\in\\mathbb{R}^N$) such that distances $d_{ij}$, $g_{ij}$ are preserved between representations\n",
    "\n",
    "Objective function: $$Y^\\ast=\\operatorname*{arg\\,min}_Y {\\sum_{i<j}{\\left|d_{ij}\\left(X\\right)-g_{ij}\\left(Y\\right)\\right|}}$$\n",
    "\n",
    "Let's not invent the wheel and use MDS implementation in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "# compute MDS embedding (2D)\n",
    "# Docs: http://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By knowing the pairwise distances between points, can we guess their coordinates (without knowing them explicitly )?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we should find pairwise distances between points in our 2D dataset:\n",
    "\n",
    "D_compressed = pdist(X_2d, metric='euclidean') # n*(n-1)/2 size\n",
    "D = squareform(D_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairwise distance matrix looks something like this: \n",
    "\n",
    "![](https://www.displayr.com/wp-content/uploads/2018/04/Distance-Matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the MDS algorithm on the pairwise distance matrix D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds_2d = MDS(n_components=2, dissimilarity='precomputed', n_jobs=-1 ).fit_transform(D) \n",
    "# it contains the embedding of original 2D data back on 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see the output of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, (a0, a1) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "a0.scatter(X_2d[:,0], X_2d[:,1], label='original data', color='steelblue', edgecolor='k', lw=0.5)\n",
    "a0.set_xlabel(r'$X$')\n",
    "a0.set_ylabel(r'$Y$')\n",
    "\n",
    "a1.scatter(mds_2d[:,0], mds_2d[:,1], label='data reconstructed with MDS', color='deepskyblue', edgecolor='k', lw=0.5)\n",
    "a1.set_xlabel('MDS axis 1')\n",
    "a1.set_ylabel('MDS axis 2')\n",
    "\n",
    "a0.legend()\n",
    "a1.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restores original coordinates up to a (i) translation, (ii) rotation and (iii) reflection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about high dimensional data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds_X = MDS(n_components=2, dissimilarity='euclidean', n_jobs=4).fit_transform(X) ## distances will be computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we get as an output of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mds_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mds_X[:,0], mds_X[:,1], color='steelblue', edgecolor='k', lw=0.5)\n",
    "plt.xlabel('MDS axis 1')\n",
    "plt.ylabel('MDS axis 2')\n",
    "plt.title('Representation of 784-dimesional space in 2D (with preserved distances)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in set(Y):\n",
    "    mask = Y==label\n",
    "    plt.scatter(mds_X[:,0][mask], mds_X[:,1][mask], label = label, edgecolor='k', lw=0.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('MDS axis 1')\n",
    "plt.ylabel('MDS axis 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.1. \n",
    "Find the relative coordinates (sketch of the map) of the European cities knowing only pairwise distances between them.\n",
    "\n",
    "Remember that MDS is a stochastic algorithm and may return different results at each run. Your embeddings may require addtional rotation and reflection to preserve geographical locations. To make your algorithm robust, apply rotations and reflections after each run of MDS. Use the information that Stockholm is directly North of Munich and Athens should be East of Lisbon. Put city labels on your scatter plot.\n",
    "\n",
    "You should get relative locations similar to these:\n",
    "![](pics/europe.jpg)\n",
    "\n",
    "To load data, use the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # Pairwise distance between European cities\n",
    "# url = 'https://media.githubusercontent.com/media/neurospin/pystatsml/master/datasets/eurodist.csv'\n",
    "# df = pd.read_csv(url)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array with cities:\n",
    "city = np.array(df[\"city\"])\n",
    "print(city)\n",
    "\n",
    "# Squareform distance matrix is here\n",
    "D = np.array(df.iloc[:, 1:]) \n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Other dimensionality reduction techniques (non-linear) t-SNE, UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [t-SNE](https://lvdmaaten.github.io/tsne/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.28 s\n",
      "Wall time: 7.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tsne_2d = TSNE(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_2d[:,0], tsne_2d[:,1], edgecolor='k', lw=0.5, color='steelblue')\n",
    "plt.title('2D t-SNE represetation of MNIST data set')\n",
    "plt.xlabel('t-SNE axis 1')\n",
    "plt.xlabel('t-SNE axis 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show with labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in set(Y):\n",
    "    mask = Y==label\n",
    "    plt.scatter(tsne_2d[:,0][mask], tsne_2d[:,1][mask], label = label, edgecolor='k', lw=0.5) \n",
    "    \n",
    "plt.title('2D t-SNE represetation of MNIST data set')\n",
    "plt.xlabel('t-SNE axis 1')\n",
    "plt.xlabel('t-SNE axis 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [UMAP](https://github.com/lmcinnes/umap) - is a very fast algorithm. My favorite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge umap-learn # download in conda venv, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import umap\n",
    "umap_2d = umap.UMAP(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=250, figsize=(4,4))\n",
    "plt.scatter(umap_2d[:,0], umap_2d[:,1], edgecolor='k', lw=0.5, color='steelblue')\n",
    "plt.title('2D UMAP represetation of MNIST data set')\n",
    "plt.xlabel('UMAP axis 1')\n",
    "plt.xlabel('UMAP axis 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show with labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in set(Y):\n",
    "    mask = Y==label\n",
    "    plt.scatter(umap_2d[:,0][mask], umap_2d[:,1][mask], marker='o', edgecolor='w', lw=2, label = label) \n",
    "plt.title('2D UMAP represetation of MNIST data set')\n",
    "plt.xlabel('UMAP axis 1')\n",
    "plt.xlabel('UMAP axis 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration: why some of the digits are clustered around other digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (umap_2d[:, 0] < 2) & (Y == 6) & (umap_2d[:, 0] > 1)\n",
    "plt.imshow(X[mask].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inspire24",
   "language": "python",
   "name": "inspire24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
